
const bayes = require('bayes');
const ngramTokenizer = require('../lib/ngram-tokenizer');
const fs = require('fs');
const path = require('path');

// Mock data loading simulation
async function runFullTest() {
    console.log('ðŸ§ª Testing Full Data Training & Persistence');

    // Initialize
    const classifier = bayes({
        tokenizer: function (text) { return ngramTokenizer.tokenize(text); }
    });

    // 1. Load User Data (Simulated from C:\Users\Teo\Downloads\classsend-data-2026-02-02.json)
    // We'll read it from the source if possible
    const specificDataPath = 'C:\\Users\\Teo\\Downloads\\classsend-data-2026-02-02.json';
    if (fs.existsSync(specificDataPath)) {
        console.log(`ðŸ“‚ Reading user data from: ${specificDataPath}`);
        const raw = fs.readFileSync(specificDataPath, 'utf-8');
        const trainingData = JSON.parse(raw);

        // Train Blacklist
        console.log(`   Training ${trainingData.blacklist.length} blacklist items...`);
        for (const item of trainingData.blacklist) {
            await classifier.learn(item.word, 'profane');
        }

        // Train Whitelist
        console.log(`   Training ${trainingData.whitelist.length} whitelist items...`);
        for (const item of trainingData.whitelist) {
            await classifier.learn(item.word, 'clean');
        }
    } else {
        console.warn('âš ï¸ User data file not found for test.');
    }

    // 2. Add some diverse clean words to balance priors (Simulating what should happen)
    const commonClean = ['ÎºÎ±Î¹', 'Ï„Î¿', 'Î³Î¹Î±', 'Î±Ï€ÏŒ', 'ÎµÎ¯Î½Î±Î¹', 'Î´ÎµÎ½', 'Î¼Îµ', 'Î¼Î¿Ï…', 'Ï€Î¿Ï…', 'Î¼Î±Î¸Î·Ï„Î®Ï‚', 'Î´Î¬ÏƒÎºÎ±Î»Î¿Ï‚', 'Ï„Î¬Î¾Î·', 'Î²Î¹Î²Î»Î¯Î¿', 'ÏƒÏ„Ï…Î»ÏŒ', 'Î¼Î¿Î»ÏÎ²Î¹', 'Î´Î¹Î±Î²Î¬Î¶Ï‰', 'Î³ÏÎ¬Ï†Ï‰'];
    for (const word of commonClean) {
        await classifier.learn(word, 'clean');
    }

    console.log('ðŸ§  Training complete.');

    // 3. Test Cases (Real world)
    const cases = [
        { text: 'ÎºÎ±Î»Î·Î¼Î­ÏÎ± ÎºÏÏÎ¹Îµ', expected: 'clean' },
        { text: 'ÎµÎ¯ÏƒÎ±Î¹ Ï€Î¿Î»Ï ÎºÎ±Î»ÏŒÏ‚', expected: 'clean' },
        { text: 'ÎµÎ¯ÏƒÎ±Î¹ Î²Î»Î¬ÎºÎ±Ï‚', expected: 'profane' },
        { text: 'Î²Î»Î±ÎºÎ­Î½Ï„Î¹Îµ', expected: 'profane' }, // Derivative
        { text: 'Î¬Î½Ï„Îµ Î³Î±Î¼Î®ÏƒÎ¿Ï…', expected: 'profane' },
        { text: 'Î³Î±Î¼ÏŽ Ï„Î¿ ÏƒÏ€Î¯Ï„Î¹ ÏƒÎ¿Ï…', expected: 'profane' },
        { text: 'Ï„Î¿ ÏƒÏ€Î¯Ï„Î¹ ÎµÎ¯Î½Î±Î¹ Ï‰ÏÎ±Î¯Î¿', expected: 'clean' } // Contextual? Naive bayes might struggle here if 'ÏƒÏ€Î¯Ï„Î¹' is neutral
    ];

    console.log('\nðŸ“Š Real-world Validation:');
    for (const c of cases) {
        const result = await classifier.categorize(c.text);

        // Manual simple check for token overlaps to understand why
        const tokens = ngramTokenizer.tokenize(c.text);

        console.log(`"${c.text}" -> ${result} [Expected: ${c.expected}]`);
    }

    // 4. Persistence Test
    const json = classifier.toJson();
    console.log(`\nðŸ’¾ Model JSON Size: ${(json.length / 1024).toFixed(2)} KB`);

    // Reload
    const reloaded = bayes.fromJson(json);
    reloaded.tokenizer = function (t) { return ngramTokenizer.tokenize(t); };

    const reloadCheck = await reloaded.categorize('Î²Î»Î¬ÎºÎ±Ï‚');
    console.log(`ðŸ”„ Reload Check ("Î²Î»Î¬ÎºÎ±Ï‚"): ${reloadCheck}`);
}

runFullTest();
